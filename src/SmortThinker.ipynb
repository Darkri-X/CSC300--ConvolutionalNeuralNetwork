{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC300 - Convolutional Neural Network Project\n",
    "> Goal: Build and train a CNN model using `tensorflow` and spectrograms given for the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data & Converting to Numbers\n",
    "1. Load Libraries\n",
    "2. Read CSV file with data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv('./spectrograms/labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are spectrograms and cannot be resizedas such we must ensure that all images are already in the required dimensions of 100x100 pixels before using them in the model. If any images do not meet this requirement, they should be excluded through ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidImage(imagePath, targetSize=(100, 100)):\n",
    "    image = Image.open(imagePath)\n",
    "    return image.size == targetSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataframe, directory, xCol, yCol, targetSize=(100, 100), batchSize=32, classMode='categorical', subset=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.directory = directory\n",
    "        self.xCol = xCol\n",
    "        self.yCol = yCol\n",
    "        self.targetSize = targetSize\n",
    "        self.batchSize = batchSize\n",
    "        self.classMode = classMode\n",
    "        self.subset = subset\n",
    "        self.datagen = ImageDataGenerator(validation_split=0.2)\n",
    "        self.validDataframe = self._filterDataframe()\n",
    "        self.samples = len(self.validDataframe)\n",
    "        self.generator = self.datagen.flow_from_dataframe(\n",
    "            dataframe=self.validDataframe,\n",
    "            directory=self.directory,\n",
    "            x_col=self.xCol,\n",
    "            y_col=self.yCol,\n",
    "            target_size=self.targetSize,\n",
    "            batch_size=self.batchSize,\n",
    "            class_mode=self.classMode,\n",
    "            subset=self.subset\n",
    "        )\n",
    "\n",
    "    def _filterDataframe(self):\n",
    "        validRows = []\n",
    "        for _, row in self.dataframe.iterrows():\n",
    "            imagePath = os.path.join(self.directory, row[self.xCol])\n",
    "            if isValidImage(imagePath, self.targetSize):\n",
    "                validRows.append(row)\n",
    "        return pd.DataFrame(validRows)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.generator[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 validated image filenames belonging to 5 classes.\n",
      "Found 500 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "trainGenerator = CustomDataGenerator(\n",
    "    dataframe=labels_df,\n",
    "    directory='.',  # base directory for the relative image paths in the CSV\n",
    "    xCol='image_path',\n",
    "    yCol='class_label',\n",
    "    targetSize=(100, 100),\n",
    "    batchSize=256,\n",
    "    classMode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validationGenerator = CustomDataGenerator(\n",
    "    dataframe=labels_df,\n",
    "    directory='.',  # base directory for the relative image paths in the CSV\n",
    "    xCol='image_path',\n",
    "    yCol='class_label',\n",
    "    targetSize=(100, 100),\n",
    "    batchSize=256,\n",
    "    classMode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential([\n",
    "    Input(shape=(100, 100, 3)),  # Specify the input shape in the Input layer\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5, activation='softmax')  # assuming 5 classes for labels a, b, c, d, e\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 720ms/step - accuracy: 0.2109 - loss: 72.4050"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 768ms/step - accuracy: 0.2101 - loss: 69.8074 - val_accuracy: 0.1660 - val_loss: 2.8205\n",
      "Epoch 2/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 734ms/step - accuracy: 0.2186 - loss: 2.1179 - val_accuracy: 0.1980 - val_loss: 1.5987\n",
      "Epoch 3/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 729ms/step - accuracy: 0.2615 - loss: 1.5800 - val_accuracy: 0.1940 - val_loss: 1.6072\n",
      "Epoch 4/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 718ms/step - accuracy: 0.2957 - loss: 1.5408 - val_accuracy: 0.2180 - val_loss: 1.5605\n",
      "Epoch 5/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 719ms/step - accuracy: 0.3395 - loss: 1.4844 - val_accuracy: 0.2580 - val_loss: 1.5778\n",
      "Epoch 6/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 705ms/step - accuracy: 0.3723 - loss: 1.4552 - val_accuracy: 0.2940 - val_loss: 1.5268\n",
      "Epoch 7/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 703ms/step - accuracy: 0.3540 - loss: 1.4437 - val_accuracy: 0.2960 - val_loss: 1.5479\n",
      "Epoch 8/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 707ms/step - accuracy: 0.3705 - loss: 1.4216 - val_accuracy: 0.2760 - val_loss: 1.5315\n",
      "Epoch 9/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 702ms/step - accuracy: 0.3940 - loss: 1.4058 - val_accuracy: 0.2760 - val_loss: 1.5797\n",
      "Epoch 10/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 703ms/step - accuracy: 0.4035 - loss: 1.3859 - val_accuracy: 0.2560 - val_loss: 1.5375\n",
      "Epoch 11/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 715ms/step - accuracy: 0.4404 - loss: 1.3367 - val_accuracy: 0.2840 - val_loss: 1.5353\n",
      "Epoch 12/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 754ms/step - accuracy: 0.4626 - loss: 1.3048 - val_accuracy: 0.3960 - val_loss: 1.4715\n",
      "Epoch 13/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 743ms/step - accuracy: 0.4914 - loss: 1.2685 - val_accuracy: 0.3180 - val_loss: 1.5471\n",
      "Epoch 14/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 740ms/step - accuracy: 0.4844 - loss: 1.2408 - val_accuracy: 0.3360 - val_loss: 1.4979\n",
      "Epoch 15/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 728ms/step - accuracy: 0.4936 - loss: 1.2428 - val_accuracy: 0.3680 - val_loss: 1.4720\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    trainGenerator,\n",
    "    steps_per_epoch=trainGenerator.samples // trainGenerator.batchSize,\n",
    "    validation_data=validationGenerator,\n",
    "    validation_steps=validationGenerator.samples // validationGenerator.batchSize,\n",
    "    epochs=15  # adjust the number of epochs as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287ms/step - accuracy: 0.3664 - loss: 1.4715\n",
      "Validation Loss: 1.4719520807266235\n",
      "Validation Accuracy: 0.36800000071525574\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "valLoss, valAcc = model.evaluate(validationGenerator)\n",
    "print(f'Validation Loss: {valLoss}')\n",
    "print(f'Validation Accuracy: {valAcc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save('imageClassifierModel.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
